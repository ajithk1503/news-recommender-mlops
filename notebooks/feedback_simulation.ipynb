{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d3384d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.recommenders.online_learner import OnlineNewsRecommender, load_user_history_json, load_news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d59cc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df=pd.read_csv(\"cleaned_data.csv\")\n",
    "user_history_dict = load_user_history_json('../data/processed/user_history.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bb0b30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(user_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2188a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OnlineNewsRecommender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e26bfc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Learn from user history\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m history \u001b[38;5;129;01min\u001b[39;00m user_history:\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     model.update_metric(history, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Recommend for user\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ajith\\Downloads\\projects\\news-recommender-mlops\\news-recommender-mlops\\notebooks\\..\\src\\recommenders\\online_learner.py:20\u001b[39m, in \u001b[36mOnlineNewsRecommender.learn_one\u001b[39m\u001b[34m(self, text, label)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn_one\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, label):\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ajith\\anaconda3\\envs\\venv\\Lib\\site-packages\\river\\compose\\pipeline.py:456\u001b[39m, in \u001b[36mPipeline.learn_one\u001b[39m\u001b[34m(self, x, y, **params)\u001b[39m\n\u001b[32m    454\u001b[39m                 sub_step.learn_one(x)\n\u001b[32m    455\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m step._supervised:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m         \u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;66;03m# Transform the data\u001b[39;00m\n\u001b[32m    458\u001b[39m x = step.transform_one(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ajith\\anaconda3\\envs\\venv\\Lib\\site-packages\\river\\feature_extraction\\vectorize.py:475\u001b[39m, in \u001b[36mTFIDF.learn_one\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn_one\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    474\u001b[39m     \u001b[38;5;66;03m# Update the document counts\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     terms = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m     \u001b[38;5;28mself\u001b[39m.dfs.update(\u001b[38;5;28mset\u001b[39m(terms))\n\u001b[32m    478\u001b[39m     \u001b[38;5;66;03m# Increment the global document counter\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ajith\\anaconda3\\envs\\venv\\Lib\\site-packages\\river\\feature_extraction\\vectorize.py:220\u001b[39m, in \u001b[36mVectorizerMixin.process_text\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.processing_steps:\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         x = \u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ajith\\anaconda3\\envs\\venv\\Lib\\site-packages\\river\\feature_extraction\\vectorize.py:28\u001b[39m, in \u001b[36mstrip_accents_unicode\u001b[39m\u001b[34m(s)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Transform accentuated unicode symbols into their ASCII counterpart.\"\"\"\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# If `s` is ASCII-compatible, then it does not contain any accented\u001b[39;00m\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# characters and we can avoid an expensive list comprehension\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[43ms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mASCII\u001b[39m\u001b[33m\"\u001b[39m, errors=\u001b[33m\"\u001b[39m\u001b[33mstrict\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m s\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeEncodeError\u001b[39;00m:\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "# Cell 4: Simulate feedback loop\n",
    "n_iterations = 100\n",
    "users = list(user_history_dict.keys())\n",
    "accuracies = []\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    user = random.choice(users)\n",
    "    user_history = user_history_dict[user]\n",
    "\n",
    "    # Learn from user history\n",
    "    for history in user_history:\n",
    "        model.learn_one(history, True)\n",
    "        model.update_metric(history, True)\n",
    "\n",
    "    # Recommend for user\n",
    "    candidate_df = news_df.sample(n=10, random_state=i)\n",
    "    top_k_news_ids = model.recommend_top_k(\n",
    "        user_history_texts=user_history,\n",
    "        candidate_news_df=candidate_df,\n",
    "        k=3\n",
    "    )\n",
    "\n",
    "    # Simulate feedback: check if any history word appears in the candidate title+abstract\n",
    "    for news_id in top_k_news_ids:\n",
    "        news_row = candidate_df[candidate_df['news_id'] == news_id].iloc[0]\n",
    "        text = news_row['title'] + \" \" + str(news_row.get('abstract', ''))\n",
    "\n",
    "        # Simulated user click if overlap in words\n",
    "        relevant = any(word in text for h in user_history for word in h.split())\n",
    "        model.learn_one(text, relevant)\n",
    "        model.update_metric(text, relevant)\n",
    "\n",
    "    # Log accuracy\n",
    "    accuracies.append(model.get_metric())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef89df5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
